---
title: "Blog post Crawling Project 2"
date: "2022-04-29"
tags: ["Python", "Scrapy", "cron job"]
excerpt: "기술 블로그를 자주 읽는 입장에서 매번 들어가서 새로운 포스트가 올라왔는지 찾아보는 것도 귀찮고 나는 여러개의 블로그를 구도하기 때문에 하나로 모아보고 싶었다. AWS의 Lambda를 이용해서 scheduled job을, 그리고 python scrapy를 이용해서 웹에서 글들을 크롤링 해오자."
author: "Wonjun Jang"
---

크론잡과 웹크롤링, 이 두가지가 잘만 합쳐지면 해볼 수 있는 프로젝트가 된다.

### scrapy

이 프로젝트에서는 python 라이브러리인 Scrapy를 사용할 것이다. 이번에 쓴 글들은 [Scrapy 공식문서](https://docs.scrapy.org/en/latest/)를 참고했다.

먼저 python3 이 설치되어있다는 가정하에 나는 macOS를 사용하기 때문에

```bash
pip3 install Scrapy
```

를 해주면 되는데 이때 주의할 점은 **꼭 가상환경을 만들어서 하도록하자**

Scrapy 설치를 완료했다면 commandline 명령어를 이용해서 우리는 Scrapy 프로젝트를 setup할 수 있다.

`spiders` 디렉토리 내에 우리가 스크래핑할 파일을 생성해 준다. 나는 `blogSpider.py`로 이름을 지어줬다.

그리고 그 안에 Scrapy 라이브러릴 import 해오고, 클래스를 정의해준다.

```python
import scrapy

class BlogSpider(scrapy.Spider):
```

공식문서를 읽어보면 클래스 안에 속성으로 name을 정의해주고 parse라고 하는 메서드를 정의해줘야한다.

```python

class BlogSpider(scrapy.Spider):
  name = 'blogspider'

  def parse(self, response):

```

그리고 parse 안을 잘 적어서 필요한 정보들을 가져와보면 된다. 나는 https://toss.tech를 스크래핑 해오기로 했다. 따라서 시작하는 url인
start_url을 정의해준다.

```python

class BlogSpider(scrapy.Spider):
  name = 'blogspider'
  start_urls = ['https://toss.tech']

  def parse(self, response):

```

그리고나서 css선택자들을 이용해서 정보들을 가져와서 parse 메서드 안에서 yield시켜주면 된다.

내가 필요로 하는 정보들은 글의 제목, 내용, 날짜, 그리고 링크이다. 따라서 각각의 해당 정보들을 가져올 수 있는 css선택자와 text값들을 가져온다.

```python
for posts in response.css('a.css-1l8x9fy'):
  try:
    yield {
      'title': posts.css('h4::text').get(),
      'exerpt': posts.css('p.css-1wl9bbt::text').get(),
      'date': posts.css('p.css-10958ez::text').get(),
      'href': self.toss_blog_url + posts.css('a').attrib['href']
    }
    self.data.append([
      posts.css('h4::text').get(),
      posts.css('p.css-1wl9bbt::text').get(),
      posts.css('p.css-10958ez::text').get(),
      self.toss_blog_url + posts.css('a').attrib['href']
    ])

    self.newData.append({
      'title': posts.css('h4::text').get(),
      'exerpt': posts.css('p.css-1wl9bbt::text').get(),
      'date': posts.css('p.css-10958ez::text').get(),
      'href': self.toss_blog_url + posts.css('a').attrib['href']
    })
  except:
    yield {
      'title': 'NONE',
      'exerpt': 'NONE',
      'date': 'NONE',
      'href': 'NONE'
    }
```

마지막에 이를 csv파일로 내보낼 것이기 때문에 이 코드를 추가해준다.

```python
with open("toss-blog-posts.csv", "a", encoding='UTF-8', newline="") as file:
      writer = csv.writer(file)
      writer.writerow(header)
      writer.writerows(self.data)
```

이렇게 해주고 명령어 crawl을 써서 원하는 정보를 가져와서 csv파일로 내보낼 수 있다.

```bash
scrapy crawl blogSpider
```

이렇게 해줘서 일단 우리가 원하는 웹에서 정보들을 긁어오는데 성공했다.

다음에는 이 정보들을 바로 Scrapy안의 pipeline을 이용해서 Database에 저장하는 것에 대해서 포스팅 하도록 하겠다.

긴 글 읽어주시느라 고생하셨습니다!~
