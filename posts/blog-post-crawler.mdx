---
title: "Blog post Crawling Project 1"
date: "2022-04-17"
tags: ["Python", "Scrapy", "cron job"]
excerpt: "기술 블로그를 자주 읽는 입장에서 매번 들어가서 새로운 포스트가 올라왔는지 찾아보는 것도 귀찮고 나는 여러개의 블로그를 구도하기 때문에 하나로 모아보고 싶었다. 따라서 나는 파이썬을 이용해서 포스트들을 클롤링 해와서 링크를 달아주는 것을 내 블로그에 같이 달아보려고 한다."
author: "Wonjun Jang"
---

나는 기술 블로그들을 많이 찾아본다. 그렇지만 기술 블로그 글들은 항상 올라오는 것도 아니고 매일 들어가서 찾아보기도 힘들다.
그렇다고 올라오면 올라왔다고 알람을 만들어도 되겠지만 그냥 이럴바에는 내가 골라보는 블로그 포스들을 모아서 하나의 페이지에서 보여주고
새로 올라올때마다 자동으로 업데이트 되도록하는 automation이 되도록 코드를 짜볼 생각이다.

## 구상하기

먼저 언어는 파이썬으로 하기로 했다. 일단 웹스크래이핑에서는 자료도 많고 코드도 비교적 간단하기 때문이다.
자바스크립트로 할 수 있으면 좋겠지만, 자바스크립트보다는 데이터분석쪽에서 주로 파이썬을 사용하기 때문에 자료를 찾기 편하다.

그래서 종이에 조금 생각했던 프로세스를 그려보았다.

<img
  src="https://zejmaqexuwsasytgncvx.supabase.co/storage/v1/object/sign/blog-post-images/blog/posts/IMG_2864-min.jpg?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1cmwiOiJibG9nLXBvc3QtaW1hZ2VzL2Jsb2cvcG9zdHMvSU1HXzI4NjQtbWluLmpwZyIsImlhdCI6MTY1MDE3NTEyMCwiZXhwIjoxOTY1NTM1MTIwfQ.FDIpokhbk2Me0QrKHQhLmvwopF2cjNzqjPwwZshuiI8"
  alt="전체적인 아키텍처"
/>

구조도를 보면 파이썬코드로 작성한 scrapper가 web에서 데이터를 가져오면 그것을 정제한다. 그때 필요한 정보들은 title, link, desc, img_url, date, source 정도이다.
사실 thumbnail이미지가 없는 포스트들도 있어서 이 부분은 어떻게 해결할지 고민중이다. 결국에는 imgUrl을 가져오지 않는 방향으로 가지 않을까 싶긴하다.

아무튼, 가져온 정보를 정제해서 python코드가 그 코드를 supabase로 만든 database에 저장한다. 그리고 그
저장된 정보들을 Nextjs에서 supabase로 호출해서 가져오는 식의 순서구조도이다.

## 봉착한 문제,

### 그렇다면 어떻게 매번 스크롤해올 것인가? 새 글이 올라왔는지를 어떻게 알 수 있을 것인가?

결국 내가 도달한 문제는 그렇다면 어떻게 새 글이 올라왔는지 알 것인가 였다.

#### 문제를 해결하기 위한 시작,

일단 문제는 **"어떻게 새 글이 올라왔는지 감지할 것인가"** 였다. 이 문제를 해결하는 여러가지 방법들이 있을 것이다.

첫번째 스크래이핑을 주기적으로 cron job을 돌리되 가장 최신 포스트의 제목이 디비에 있으면 스크래이핑을 하지 않고 넘어간다.

두번째 웹사이트에 webhook같은 것을 걸어서 새 글이 올라오면 내쪽으로 신호가 trigger되서 스크래이핑을 다시 해오게 한다.

나에게는 두번째 옵션이 가장 ideal하다, 하지만 이것이 기술적으로 정말 가능한지는 찾아봐야할 것 같다.

### 주기적으로 cron을 돌려서 확인하는 방법

cron job을 혹시 모르는 사람이 있을까봐 간단하게 설명을하자면, 내가 원할때 나 대신 자동으로 실행할 수 있도록 해주는 작업을
cron job이라고 한다. 그러니까 내가 매일 새벽3시 뭔가를 하는 프로그램이 있다고 한다면 그 프로그램은 cron job을 돌리면 된다.

보통 크론잡을 돌리려면 돌아가는 배포된 서버가 필요하다. 따라서 우리는 작은 서버를 배포하거나 가지고 있으면 되는데
대부분의 사람들은 이것을 클라우드를 통해서 할 것이고 나 또한 그럴 것이다.

### Python 라이브러리 사용, Scrapy, Crontab

- [Scrapy](https://docs.scrapy.org/en/latest/)
- [crontab](https://gitlab.com/doctormo/python-crontab/-/blob/master/README.rst)

이 두가지 라이브러리를 이용해서 어딘가에서 가져오는 것을 구체화할 수 있을 것으로 보인다. 먼저 `Scrapy`라는 라이브러리를 이용해서
여러개의 웹에서 글을 갖고오는 코드를 짜보도록 하자.
